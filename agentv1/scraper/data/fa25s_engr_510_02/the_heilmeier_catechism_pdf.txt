The Heilmeier Catechism: A Practical Guide for Pitch Design and Decision Making What it is and why it matters The Heilmeier Catechism is a short set of questions created in the 1970s by George H. Heilmeier, then director of DARPA, to vet ambitious research proposals. Its power is not in fancy theory, but in forcing clear, plain answers to the fundamentals: What are you doing, why does it matter, how will you know it worked, and what will it take? Today, venture capitalists, product teams, nonprofits, and policy labs use it to shape pitches and make funding decisions.
The catechism gives you a simple, repeatable framework to design a compelling pitch and to evaluate others' pitches. It helps you communicate value, anticipate objections, make trade-offs explicit, and define success in measurable terms.
The questions and what a good answer looks like 1. What problem are you trying to solve? State the objective clearly, in plain language.
Avoid solution-first framing and avoid jargon. A good answer is specific, observable, and user-centered: "Reduce line wait times at campus dining halls by 50% during lunch hours." 2. How is it done today, and what are the limits? Describe existing alternatives, including "do nothing." Compare cost, performance, experience, and constraints. A good answer is fair and grounded: "Students use a live camera feed or guess; current app estimates lag 10-15 minutes and don't account for mobile orders." 3. What's new in your approach, and why will it work? Explain your insight, mechanism, or enabling change (data, tech, policy, behavior). A good answer links novelty to outcomes and cites evidence or testable hypotheses: "We combine point-of-sale data and Wi-Fi presence to predict queue length to within ±2 minutes; a 2-week pilot on two lines showed 85% accuracy." 4. Who cares? If you succeed, who benefits and by how much? Quantify users, buyers, and stakeholders. A good answer ties impact to metrics people care about: "10,000 daily diners save 6 minutes each; dining reduces crowding complaints by 70%; facilities can stagger staffing more efficiently."
 5. What are the risks and payoffs? Identify technical, market, operational, ethical, and regulatory risks. Be honest about unknowns. A good answer prioritizes risks and proposes mitigation: "Predictive accuracy may drop during special events; we'll add an anomaly flag and manual override. Privacy risks mitigated by device-level aggregation." 6. What will it take? Estimate resources: people, budget, data, hardware/software, partnerships. A good answer is scoped and staged: "Three students part-time, $8,000 for cloud and sensors, two data-sharing MOUs. Stage 1: two dining halls; Stage 2: campuswide." 7. How long will it take? Present a credible plan and schedule with milestones. A good answer includes checkpoints: "Week 4: data access confirmed; Week 8: prototype; Week 12: pilot; Week 16: evaluation and go/no-go." 8. How will you measure success? Define concrete metrics and "exams." A good answer distinguishes leading indicators from outcomes: "Midterm exam: prediction error < ±3 minutes for 80% of intervals; Final exam: average wait time reduced by 40% with >30% weekly active users." How it shapes a strong pitch You can map a compelling pitch directly onto the catechism:
- Hook: The problem and who feels it (Problem; Who cares?).
- Landscape: Current approaches and gaps (How it's done today).
- Your solution: What's new and why now (What's new; Why it will work).
- Impact: Benefits and evidence (Who cares; Payoffs).
- Feasibility: Plan, resources, and timeline (What will it take; How long).
- Risk and ethics: Assumptions, uncertainties, safeguards (Risks).
- Success criteria: Metrics, milestones, and decision points (How you measure success).
- The ask: Funding, access, partnerships aligned to milestones.
Using it for better decisions The catechism is also a decision tool:
 - It makes proposals comparable. Two very different ideas can be judged on the same dimensions: problem clarity, novelty, impact, cost, risk, and measurability.
- It builds in stage gates. Milestones and "exams" let you make go/kill/pivot decisions early, reducing sunk-cost bias.
- It surfaces assumptions. By stating what must be true, teams can design tests to validate or refute key hypotheses quickly.
- It aligns stakeholders. Sponsors, users, and builders can agree on outcomes and evidence before work starts.
A quick mini-example Suppose a team proposes "Refill," a smart bottle cap that logs hydration and nudges users to drink water.
- Problem: Many students report fatigue and headaches related to dehydration; current reminders are generic and ignored.
- Today's alternatives: Phone reminders and fitness apps lack context; smart bottles exist but are expensive and fragile.
- What's new and why it works: Use low-cost acoustic sensing in the cap to detect sips, plus context from class schedules to time nudges. Early test with 20 students increased water intake by 22%.
- Who cares: Students seeking wellness; athletics; health services. If successful, could reduce reported headaches by 30% and improve perceived alertness in afternoon classes.
- Risks/payoffs: Sensor accuracy with carbonated drinks; device durability; novelty wearing off. Payoff is a <$20 accessory with high retention if nudges are personalized.
- What it takes: $12,000 for prototypes; one EE, one UX, one data student; pilot with athletics department.
- Timeline: 4 months to pilot; decision at 8 weeks based on sensor accuracy and daily active use.
- Success metrics: >85% sip detection accuracy; >50% weekly active users at week 8; selfreported hydration up 15% vs. control.
Common pitfalls and how to avoid them - Jargon and buzzwords. Replace "AI-powered" with the specific model, data, and expected error rates.
 - Vague problems. "Improve student life" is not a problem; "Reduce 30-minute peak wait times at dining hall X" is.
- Hand-waving on evidence. Cite pilots, analogous cases, or plan a quick experiment to test the riskiest assumption.
- Wishful timelines and budgets. Break work into stages and include buffer for approvals, integration, and testing.
- Ignoring adoption and transition. Identify who will adopt, pay, and maintain the solution;
include a transition plan.
- Overlooking ethics and compliance. Address privacy, safety, accessibility, and potential misuse upfront.